{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "roopagmlresourcegp"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_exists')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "emp"
						},
						{
							"dataset": {
								"referenceName": "Deptdataset",
								"type": "DatasetReference"
							},
							"name": "dept"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "exists1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"source(output(",
						"          DEPARTMENT_ID as string,",
						"          DEPARTMENT_NAME as string,",
						"          MANAGER_ID as string,",
						"          LOCATION_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> dept",
						"dept, emp exists(dept@DEPARTMENT_ID == emp@DEPARTMENT_ID,",
						"     negate:true,",
						"     broadcast: 'auto')~> exists1",
						"exists1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_flowlet')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flowlet1",
							"flowlet": {
								"referenceName": "flowlet1",
								"type": "DataFlowReference",
								"parameters": {}
							}
						},
						{
							"name": "select1"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"derivedColumn1 compose(mapColumn(",
						"          EMPLOYEE_ID,",
						"          {FIRST_NAME     } = FIRST_NAME,",
						"          DEPARTMENT_ID,",
						"          DEPARTMENT_NAME = dname",
						"     ),",
						"     composition: 'flowlet1') ~> flowlet1@(output1)",
						"source1 select(mapColumn(",
						"          EMPLOYEE_ID,",
						"          FIRST_NAME,",
						"          DEPARTMENT_ID",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 derive(dname = 'dummy') ~> derivedColumn1",
						"flowlet1@output1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_inlinedataset')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"linkedService": {
								"referenceName": "GML_Blob_Storage",
								"type": "LinkedServiceReference"
							},
							"name": "Inlineemp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          { ID} as short,",
						"          LOC as string,",
						"          SAL as integer",
						"     ),",
						"     useSchema: false,",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     format: 'delimited',",
						"     container: 'roopatpgcontainer',",
						"     folderPath: 'Input',",
						"     fileName: 'Customer.txt',",
						"     columnDelimiter: ',',",
						"     escapeChar: '\\\\',",
						"     quoteChar: '\\\"',",
						"     columnNamesAsHeader: true) ~> Inlineemp",
						"Inlineemp sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_lookuptransformation')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Deptdataset",
								"type": "DatasetReference"
							},
							"name": "dept"
						},
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "lookup1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          DEPARTMENT_ID as string,",
						"          DEPARTMENT_NAME as string,",
						"          MANAGER_ID as string,",
						"          LOCATION_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> dept",
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"dept, emp lookup(dept@DEPARTMENT_ID == emp@DEPARTMENT_ID,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookup1",
						"lookup1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_parsetransform')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable2",
								"type": "DatasetReference"
							},
							"name": "empjson"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "parseskilla"
						},
						{
							"name": "parseaddress"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          ename as string,",
						"          skills as string,",
						"          address as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     isolationLevel: 'READ_UNCOMMITTED',",
						"     format: 'table') ~> empjson",
						"empjson parse(skills = skills ? (skill1 as string,",
						"          skill2 as string,",
						"          skill3 as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseskilla",
						"parseskilla parse(address = address ? (city as string,",
						"          country as string),",
						"     format: 'json',",
						"     documentForm: 'singleDocument') ~> parseaddress",
						"parseaddress sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['parse.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empid,",
						"          ename,",
						"          skill1 = skills.skill1,",
						"          skill2 = skills.skill2,",
						"          city = address.city,",
						"          country = address.country",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_pivot')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "pivot_emp_data",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "pivot1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EmpID as string,",
						"          FirstName as string,",
						"          LastName as string,",
						"          StartDate as string,",
						"          ExitDate as string,",
						"          Title as string,",
						"          Supervisor as string,",
						"          ADEmail as string,",
						"          BusinessUnit as string,",
						"          EmployeeStatus as string,",
						"          EmployeeType as string,",
						"          PayZone as string,",
						"          EmployeeClassificationType as string,",
						"          TerminationType as string,",
						"          TerminationDescription as string,",
						"          DepartmentType as string,",
						"          Division as string,",
						"          DOB as string,",
						"          State as string,",
						"          JobFunctionDescription as string,",
						"          GenderCode as string,",
						"          LocationCode as string,",
						"          RaceDesc as string,",
						"          MaritalDesc as string,",
						"          {Performance Score} as string,",
						"          {Current Employee Rating} as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"emp pivot(groupBy(DepartmentType),",
						"     pivotBy(GenderCode),",
						"     {} = count(EmpID),",
						"     columnNaming: '$N$V',",
						"     lateral: true) ~> pivot1",
						"pivot1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_select')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"emp select(mapColumn(",
						"          EMP_ID = EMPLOYEE_ID,",
						"          FIRST_NAME,",
						"          LAST_NAME,",
						"          EMAIL,",
						"          MANAGER_ID,",
						"          PHONE_NO = PHONE_NUMBER",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SELECTTRANSFORM.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_sort')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "sort1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"emp sort(desc(FIRST_NAME, true)) ~> sort1",
						"sort1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['sort.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_stringify')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Json",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "stringify1"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          skills as string[],",
						"          contact as (mobile as string, landline as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> emp",
						"emp stringify(newcontact = contact ? string,",
						"     format: 'json') ~> stringify1",
						"stringify1 derive(newcontact = toString(newcontact)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['stringify.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          name,",
						"          skills,",
						"          newcontact",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_validateschema')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as string,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['validateschema,txt'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_windowtransformation')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "window1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as integer,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as integer,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> emp",
						"emp window(over(DEPARTMENT_ID),",
						"     asc(SALARY, true),",
						"     Rank = denseRank()) ~> window1",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow_writecachesink')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Empdataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          EMPLOYEE_ID as string,",
						"          FIRST_NAME as string,",
						"          LAST_NAME as string,",
						"          EMAIL as string,",
						"          PHONE_NUMBER as string,",
						"          HIRE_DATE as string,",
						"          JOB_ID as string,",
						"          SALARY as integer,",
						"          COMMISSION_PCT as string,",
						"          MANAGER_ID as string,",
						"          DEPARTMENT_ID as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(maxsal = max(SALARY)) ~> aggregate1",
						"aggregate1 sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_logdetails')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dummy",
								"type": "DatasetReference"
							},
							"name": "dummy"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "log_file",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     adfname as string,",
						"     pipename as string,",
						"     status as string,",
						"     runid as string,",
						"     triggername as string,",
						"     logfilename as string",
						"}",
						"source(output(",
						"          Column_1 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> dummy",
						"dummy derive(adfname = $adfname,",
						"          pipename = $pipename) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:[($logfilename)],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_logdetails_append')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "dummy",
								"type": "DatasetReference"
							},
							"name": "dummy"
						},
						{
							"dataset": {
								"referenceName": "log_file",
								"type": "DatasetReference"
							},
							"name": "logappend"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "log_file",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn1"
						},
						{
							"name": "union1"
						}
					],
					"scriptLines": [
						"parameters{",
						"     adfname as string,",
						"     pipename as string,",
						"     status as string,",
						"     runid as string,",
						"     triggername as string,",
						"     logfilename as string",
						"}",
						"source(output(",
						"          Column_1 as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> dummy",
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     wildcardPaths:[('roopatpgcontainer/log'+$logfilename)]) ~> logappend",
						"dummy derive(adfname = $adfname,",
						"          pipename = $pipename) ~> derivedColumn1",
						"derivedColumn1, logappend union(byName: true)~> union1",
						"union1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:[($logfilename)],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/handleerror')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "handleerror_dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "AzureSqlTable_sales",
								"type": "DatasetReference"
							},
							"name": "sink2"
						},
						{
							"dataset": {
								"referenceName": "AzureSqlTable_sales_baddata",
								"type": "DatasetReference"
							},
							"name": "sink3"
						}
					],
					"transformations": [
						{
							"name": "assert1"
						},
						{
							"name": "split1"
						},
						{
							"name": "filename"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          salesdate as string,",
						"          item as string,",
						"          country as string,",
						"          quantity as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 assert(expectTrue(isDate(salesdate,'dd-mm-yyyy'), false, 'date check'),",
						"     expectTrue(isInteger(quantity), false, 'quantity check')) ~> assert1",
						"assert1 split(isError(),",
						"     disjoint: false) ~> split1@(badrows, goodrows)",
						"split1@badrows derive(filename = 'sales.csv') ~> filename",
						"split1@goodrows derive(filename = 'sales.csv',",
						"          new_salesdate = toDate(salesdate, 'dd-mm-yyyy'),",
						"          new_quantity = toInteger(quantity)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          salesdate as date,",
						"          item as string,",
						"          country as string,",
						"          quantity as integer,",
						"          filename as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          salesdate = new_salesdate,",
						"          item,",
						"          country,",
						"          quantity = new_quantity,",
						"          filename",
						"     )) ~> sink2",
						"filename sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          salesdate as string,",
						"          item as string,",
						"          country as string,",
						"          quantity as string,",
						"          filename as string",
						"     ),",
						"     deletable:false,",
						"     insertable:true,",
						"     updateable:false,",
						"     upsertable:false,",
						"     format: 'table',",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     errorHandlingOption: 'stopOnFirstError',",
						"     mapColumn(",
						"          salesdate,",
						"          item,",
						"          country,",
						"          quantity,",
						"          filename",
						"     )) ~> sink3"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/removeduplicates')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "customer_dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "output2dataset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregate1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          { ID} as string,",
						"          LOC as string,",
						"          SAL as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 aggregate(each(match(name!='sal'), $$ = first($$))) ~> aggregate1",
						"aggregate1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          Name as string,",
						"          Category as string,",
						"          Status as string,",
						"          Error as string",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/runningtotal')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "customer_dataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Outputdataset_noparam",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "window1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Name as string,",
						"          { ID} as string,",
						"          LOC as string,",
						"          SAL as integer",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 window(asc(SAL, true),",
						"     new_sal = sum(SAL)) ~> window1",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Dataflow_join')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflowJoin",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Emp": {},
									"Dept": {},
									"Output": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Dflow_cachesink')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "dataflow_cahcesink",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"empdb": {},
									"empblob": {},
									"cachedmaxempid": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/auditlog')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Set variable1",
						"type": "SetVariable",
						"dependsOn": [],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"variableName": "filename",
							"value": {
								"value": "@concat(formatDateTime(utcnow()),'_log.csv')",
								"type": "Expression"
							}
						}
					},
					{
						"name": "Get Metadata1",
						"type": "GetMetadata",
						"dependsOn": [
							{
								"activity": "Set variable1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "log_file",
								"type": "DatasetReference",
								"parameters": {
									"filename": {
										"value": "@variables('filename')",
										"type": "Expression"
									}
								}
							},
							"fieldList": [
								"exists"
							],
							"storeSettings": {
								"type": "AzureBlobStorageReadSettings",
								"recursive": true,
								"enablePartitionDiscovery": false
							},
							"formatSettings": {
								"type": "DelimitedTextReadSettings"
							}
						}
					},
					{
						"name": "If Condition1",
						"type": "IfCondition",
						"dependsOn": [
							{
								"activity": "Get Metadata1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"userProperties": [],
						"typeProperties": {
							"expression": {
								"value": "@activity('Get Metadata1').output.exists",
								"type": "Expression"
							},
							"ifFalseActivities": [
								{
									"name": "df_logdetails",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "df_logdetails",
											"type": "DataFlowReference",
											"parameters": {
												"adfname": {
													"value": "'@{pipeline().DataFactory}'",
													"type": "Expression"
												},
												"pipename": {
													"value": "'@{pipeline().Pipeline}'",
													"type": "Expression"
												},
												"status": {
													"value": "'@{string('success')}'",
													"type": "Expression"
												},
												"runid": {
													"value": "'@{pipeline().RunId}'",
													"type": "Expression"
												},
												"triggername": {
													"value": "'@{pipeline().TriggerName}'",
													"type": "Expression"
												},
												"logfilename": {
													"value": "'@{variables('filename')}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"dummy": {},
												"sink1": {
													"filename": {
														"value": "@variables('filename')",
														"type": "Expression"
													}
												}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine"
									}
								}
							],
							"ifTrueActivities": [
								{
									"name": "df_logdetails_append",
									"type": "ExecuteDataFlow",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"dataflow": {
											"referenceName": "df_logdetails_append",
											"type": "DataFlowReference",
											"parameters": {
												"adfname": {
													"value": "'@{pipeline().DataFactory}'",
													"type": "Expression"
												},
												"pipename": {
													"value": "'@{pipeline().Pipeline}'",
													"type": "Expression"
												},
												"status": {
													"value": "'@{'success'}'",
													"type": "Expression"
												},
												"runid": {
													"value": "'@{pipeline().RunId}'",
													"type": "Expression"
												},
												"triggername": {
													"value": "'@{pipeline().TriggerId}'",
													"type": "Expression"
												},
												"logfilename": {
													"value": "'@{variables('filename')}'",
													"type": "Expression"
												}
											},
											"datasetParameters": {
												"dummy": {},
												"logappend": {
													"filename": {
														"value": "@variables('filename')",
														"type": "Expression"
													}
												},
												"sink1": {
													"filename": {
														"value": "@variables('filename')",
														"type": "Expression"
													}
												}
											}
										},
										"staging": {},
										"compute": {
											"coreCount": 8,
											"computeType": "General"
										},
										"traceLevel": "Fine"
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"variables": {
					"filename": {
						"type": "String"
					}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_logdetails')]",
				"[concat(variables('factoryId'), '/dataflows/df_logdetails_append')]"
			]
		}
	]
}